{\rtf1\ansi\ansicpg1252\cocoartf2708
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\fs24 \cf0 Mukund Sharma \
https://github.com/abhinav-bhardwaj/IoT-Network-Intrusion-Detection-System-UNSW-NB15\
\
Split dataset into three parts : [So 3 different notebooks you need to make ]\
70-30\
75-25\
80-20\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 Implement these feature selection technique. You need to combine them into a single Google Colab notebook later on. At the start you can practise individual notebooks. Then later on we need to combine all feature selection in to a single Google colab notebook.\
\
Mukund Sharma \
https://github.com/anujdutt9/Feature-Selection-for-Machine-Learning\
https://github.com/ttungl/feature-selection-for-machine-learning\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
Implement all these algos for classification for your dataset which is given [Single Colab Notebook. Share On Github as Pvt notebook mayank265@gmail.com]\
[Implement all algos for 70-30, 75-25, 80-20]\
\
1. AdaBoost\
2. Artificial Neural Network (ANN)\
3. Bagging (Bootstrap Aggregating)\
4. Bayesian network classifiers\
5. Categorical Naive Bayes\
6. Convolutional Neural Network (CNN)\
7. Decision Trees\
8. Deep Neural Network (DNN)\
9. Gaussian Process Classification\
10. Gradient Boosting (e.g., XGBoost & LightGBM)\
11. J48\
12. K-Nearest neighbour (KNN) Classifier\
13. Linear Discriminant Analysis (LDA)\
14. Logistic Regression\
15. Multiclass Classification with One-vs-Rest (OvR) or One-vs-One (OvO)\
16. Naive Bayes\
17. Passive Aggressive Classifier\
18. Perceptron\
19. Quadratic Discriminant Analysis (QDA)\
20. Random Forest\
21. Recurrent Neural Network (RNN)\
22. Ridge Classifier\
23. Sparse auto-encoder\
24. Support Vector Machines (SVM)\
\
These are metrics you need to compute for the above ML methods\
1. Accuracy\
2. ANNOVA (Statistical technique)\
3. APIM Score\
4. AUC - ROC Area\
5. Balanced Accuracy (BACC)\
6. capture rate\
7. Classification accuracy\
8. Cohen Kappa score\
9. Conditional Average Treatment Effect (CPE)\
10. Confusion Matrix\
11. Correct classification rate\
12. Correlation Coefficient\
13. coverage capture rate\
14. coverage rate\
15. Degree of Dependency (DoD)\
16. Detection rate\
17. Detection time\
18. F1-Score\
19. Fall-out rate\
20. False alarm rate\
21. False Negative rate (FNR)\
22. False Positive rate (FPR)\
23. Feature importance scores\
24. FFS (Fast Feature Selection)\
25. F-measure\
26. Fraction of positives correctly identified (FPC)\
27. F-Ratio\
28. F-test\
29. GAN (Generative Adversarial Network) loss\
30. GINI Index\
31. G-Mean\
32. Hamming loss\
33. Hit Rate\
34. In addition compute the following metrics too.\
35. Jaccard score\
36. Kappa Statistic\
37. Kolmogorov-Smirnov (KS -statistic)\
38. Least Square Regression Error\
39. LOF (Local Outlier Factor) Score\
40. Log loss\
41. Matthews correlation coefficient (MCC)\
42. Maximal Information Compression Index\
43. Mean absolute error (MSE)\
44. Mean estimate\
45. Model predictive power\
46. Negative Predictive Value (NPV)\
47. Positive predictive value (PPV)\
48. Precision\
49. Prediction time\
50. Recall\
51. Relative Absolute Error (RSE)\
52. Root mean squared error (RMSE)\
53. Selectivity\
54. Sensitivity\
55. Specificity\
56. Sum of Squares\
57. Testing time\
58. Training time\
59. True negative rate (TNR)\
60. True positive rate (TPR)\
61. True reject rate\
\
Guys for the statistics it may happen that some of the above metrics might not able just ignore those}